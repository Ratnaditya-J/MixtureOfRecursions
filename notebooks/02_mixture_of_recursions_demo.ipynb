{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture-of-Recursions (MoR) Implementation Demo\n",
    "\n",
    "This notebook demonstrates the implementation of the Mixture-of-Recursions model as described in the paper:\n",
    "\"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation\"\n",
    "\n",
    "## Key Features:\n",
    "- Recursive transformer layers with parameter sharing\n",
    "- Adaptive token-level computation via routing\n",
    "- Selective attention and KV caching\n",
    "- Efficiency optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from models.mor_model import MixtureOfRecursions, MoRConfig\n",
    "from utils.config import Config\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Configuration and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model configuration\n",
    "config = MoRConfig(\n",
    "    vocab_size=50257,  # GPT-2 vocabulary size\n",
    "    hidden_size=512,   # Smaller for demo\n",
    "    num_attention_heads=8,\n",
    "    num_hidden_layers=6,\n",
    "    max_recursion_depth=4,\n",
    "    min_recursion_depth=1,\n",
    "    use_kv_sharing=True,  # Enable KV sharing variant\n",
    "    router_hidden_size=128\n",
    ")\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "for key, value in config.__dict__.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MixtureOfRecursions(config).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Model statistics\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward Pass Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test input\n",
    "test_text = \"The quick brown fox jumps over the lazy dog. This sentence demonstrates adaptive computation.\"\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "print(f\"Input text: {test_text}\")\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Tokens: {tokenizer.convert_ids_to_tokens(input_ids[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "logits = outputs[\"logits\"]\n",
    "recursion_depths = outputs[\"recursion_depths\"]\n",
    "router_loss = outputs[\"router_loss\"]\n",
    "\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"Recursion depths shape: {recursion_depths.shape}\")\n",
    "print(f\"Router loss: {router_loss.item():.4f}\")\n",
    "\n",
    "# Show recursion depths per token\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "depths = recursion_depths[0].cpu().numpy()\n",
    "\n",
    "print(\"\\nToken-level recursion depths:\")\n",
    "for token, depth in zip(tokens, depths):\n",
    "    if token != tokenizer.pad_token:\n",
    "        print(f\"  {token:15} -> depth {depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recursion Depth Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze recursion depth patterns across different inputs\n",
    "test_sentences = [\n",
    "    \"Simple sentence.\",\n",
    "    \"This is a more complex sentence with multiple clauses and sophisticated vocabulary.\",\n",
    "    \"The quick brown fox jumps.\",\n",
    "    \"In the realm of artificial intelligence, the development of sophisticated language models represents a significant milestone.\",\n",
    "    \"Hello world!\"\n",
    "]\n",
    "\n",
    "depth_analysis = []\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        depths = outputs[\"recursion_depths\"][0].cpu().numpy()\n",
    "        \n",
    "        # Only consider non-padding tokens\n",
    "        valid_depths = depths[attention_mask[0].cpu().numpy() == 1]\n",
    "        \n",
    "        depth_analysis.append({\n",
    "            'sentence': sentence,\n",
    "            'length': len(sentence.split()),\n",
    "            'avg_depth': np.mean(valid_depths),\n",
    "            'max_depth': np.max(valid_depths),\n",
    "            'min_depth': np.min(valid_depths),\n",
    "            'depth_std': np.std(valid_depths),\n",
    "            'depths': valid_depths\n",
    "        })\n",
    "\n",
    "# Display analysis\n",
    "print(\"Recursion Depth Analysis:\")\n",
    "print(\"-\" * 80)\n",
    "for analysis in depth_analysis:\n",
    "    print(f\"Sentence: {analysis['sentence'][:50]}{'...' if len(analysis['sentence']) > 50 else ''}\")\n",
    "    print(f\"  Length: {analysis['length']} words\")\n",
    "    print(f\"  Avg depth: {analysis['avg_depth']:.2f} Â± {analysis['depth_std']:.2f}\")\n",
    "    print(f\"  Depth range: {analysis['min_depth']} - {analysis['max_depth']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize recursion depth patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Average depth vs sentence length\n",
    "lengths = [a['length'] for a in depth_analysis]\n",
    "avg_depths = [a['avg_depth'] for a in depth_analysis]\n",
    "\n",
    "axes[0, 0].scatter(lengths, avg_depths, s=100, alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Sentence Length (words)')\n",
    "axes[0, 0].set_ylabel('Average Recursion Depth')\n",
    "axes[0, 0].set_title('Recursion Depth vs Sentence Complexity')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Depth distribution histogram\n",
    "all_depths = np.concatenate([a['depths'] for a in depth_analysis])\n",
    "axes[0, 1].hist(all_depths, bins=range(config.min_recursion_depth, config.max_recursion_depth + 2), \n",
    "                alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Recursion Depth')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Overall Depth Distribution')\n",
    "axes[0, 1].set_xticks(range(config.min_recursion_depth, config.max_recursion_depth + 1))\n",
    "\n",
    "# 3. Depth variance analysis\n",
    "depth_stds = [a['depth_std'] for a in depth_analysis]\n",
    "axes[1, 0].bar(range(len(depth_analysis)), depth_stds, alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Sentence Index')\n",
    "axes[1, 0].set_ylabel('Depth Standard Deviation')\n",
    "axes[1, 0].set_title('Depth Variability per Sentence')\n",
    "axes[1, 0].set_xticks(range(len(depth_analysis)))\n",
    "\n",
    "# 4. Token-level depth visualization for longest sentence\n",
    "longest_idx = np.argmax(lengths)\n",
    "longest_analysis = depth_analysis[longest_idx]\n",
    "axes[1, 1].plot(longest_analysis['depths'], 'o-', linewidth=2, markersize=6)\n",
    "axes[1, 1].set_xlabel('Token Position')\n",
    "axes[1, 1].set_ylabel('Recursion Depth')\n",
    "axes[1, 1].set_title(f'Token-level Depths: \"{longest_analysis[\"sentence\"][:30]}...\"')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim(config.min_recursion_depth - 0.5, config.max_recursion_depth + 0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare computational efficiency with different recursion depths\n",
    "import time\n",
    "\n",
    "def measure_throughput(model, input_ids, attention_mask, num_runs=10):\n",
    "    \"\"\"Measure model throughput.\"\"\"\n",
    "    # Warmup\n",
    "    for _ in range(3):\n",
    "        with torch.no_grad():\n",
    "            _ = model(input_ids, attention_mask)\n",
    "    \n",
    "    # Measure\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "    \n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_runs\n",
    "    tokens_per_second = (input_ids.shape[0] * input_ids.shape[1]) / avg_time\n",
    "    \n",
    "    return avg_time, tokens_per_second, outputs\n",
    "\n",
    "# Test with different sequence lengths\n",
    "sequence_lengths = [64, 128, 256, 512]\n",
    "throughput_results = []\n",
    "\n",
    "for seq_len in sequence_lengths:\n",
    "    if seq_len > config.max_position_embeddings:\n",
    "        continue\n",
    "        \n",
    "    # Create test input\n",
    "    test_input = torch.randint(0, config.vocab_size, (4, seq_len)).to(device)\n",
    "    test_mask = torch.ones_like(test_input).to(device)\n",
    "    \n",
    "    avg_time, throughput, outputs = measure_throughput(model, test_input, test_mask)\n",
    "    avg_depth = outputs[\"recursion_depths\"].float().mean().item()\n",
    "    \n",
    "    throughput_results.append({\n",
    "        'seq_len': seq_len,\n",
    "        'avg_time': avg_time,\n",
    "        'throughput': throughput,\n",
    "        'avg_depth': avg_depth\n",
    "    })\n",
    "    \n",
    "    print(f\"Seq len {seq_len:3d}: {throughput:6.1f} tokens/sec, {avg_time*1000:5.1f}ms, avg depth: {avg_depth:.2f}\")\n",
    "\n",
    "# Visualize throughput results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "seq_lens = [r['seq_len'] for r in throughput_results]\n",
    "throughputs = [r['throughput'] for r in throughput_results]\n",
    "avg_depths = [r['avg_depth'] for r in throughput_results]\n",
    "\n",
    "ax1.plot(seq_lens, throughputs, 'o-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Sequence Length')\n",
    "ax1.set_ylabel('Throughput (tokens/sec)')\n",
    "ax1.set_title('Model Throughput vs Sequence Length')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(seq_lens, avg_depths, 's-', linewidth=2, markersize=8, color='orange')\n",
    "ax2.set_xlabel('Sequence Length')\n",
    "ax2.set_ylabel('Average Recursion Depth')\n",
    "ax2.set_title('Recursion Depth vs Sequence Length')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Demonstration (Mini-batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate training on a small batch\n",
    "model.train()\n",
    "\n",
    "# Create training data\n",
    "training_texts = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Machine learning is fascinating.\",\n",
    "    \"Recursive models can be efficient.\",\n",
    "    \"Attention mechanisms are powerful.\"\n",
    "]\n",
    "\n",
    "# Tokenize training data\n",
    "train_inputs = tokenizer(training_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
    "train_input_ids = train_inputs[\"input_ids\"].to(device)\n",
    "train_attention_mask = train_inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "# Create labels (shifted input_ids)\n",
    "labels = train_input_ids.clone()\n",
    "labels[train_attention_mask == 0] = -100\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "router_losses = []\n",
    "lm_losses = []\n",
    "\n",
    "print(\"Training demonstration (10 steps):\")\n",
    "for step in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(train_input_ids, train_attention_mask)\n",
    "    logits = outputs[\"logits\"]\n",
    "    router_loss = outputs[\"router_loss\"]\n",
    "    \n",
    "    # Compute language modeling loss\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    \n",
    "    lm_loss = nn.CrossEntropyLoss()(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1)\n",
    "    )\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = lm_loss + router_loss\n",
    "    \n",
    "    # Backward pass\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Store losses\n",
    "    losses.append(total_loss.item())\n",
    "    router_losses.append(router_loss.item())\n",
    "    lm_losses.append(lm_loss.item())\n",
    "    \n",
    "    if step % 2 == 0:\n",
    "        avg_depth = outputs[\"recursion_depths\"].float().mean().item()\n",
    "        print(f\"Step {step:2d}: Total={total_loss.item():.4f}, LM={lm_loss.item():.4f}, Router={router_loss.item():.4f}, Depth={avg_depth:.2f}\")\n",
    "\n",
    "# Plot training losses\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(losses, label='Total Loss', linewidth=2)\n",
    "ax1.plot(lm_losses, label='LM Loss', linewidth=2)\n",
    "ax1.plot(router_losses, label='Router Loss', linewidth=2)\n",
    "ax1.set_xlabel('Training Step')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Losses')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Show final recursion depths\n",
    "final_depths = outputs[\"recursion_depths\"][0].cpu().numpy()\n",
    "ax2.plot(final_depths, 'o-', linewidth=2, markersize=6)\n",
    "ax2.set_xlabel('Token Position')\n",
    "ax2.set_ylabel('Recursion Depth')\n",
    "ax2.set_title('Final Recursion Depths (First Sample)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(config.min_recursion_depth - 0.5, config.max_recursion_depth + 0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model architecture and parameter distribution\n",
    "def analyze_model_parameters(model):\n",
    "    \"\"\"Analyze parameter distribution across model components.\"\"\"\n",
    "    param_stats = {}\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if len(list(module.children())) == 0:  # Leaf modules only\n",
    "            num_params = sum(p.numel() for p in module.parameters())\n",
    "            if num_params > 0:\n",
    "                component = name.split('.')[0] if '.' in name else name\n",
    "                if component not in param_stats:\n",
    "                    param_stats[component] = 0\n",
    "                param_stats[component] += num_params\n",
    "    \n",
    "    return param_stats\n",
    "\n",
    "param_stats = analyze_model_parameters(model)\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Parameter distribution pie chart\n",
    "components = list(param_stats.keys())\n",
    "param_counts = list(param_stats.values())\n",
    "\n",
    "ax1.pie(param_counts, labels=components, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Parameter Distribution by Component')\n",
    "\n",
    "# Parameter counts bar chart\n",
    "ax2.bar(components, [p/1000 for p in param_counts])\n",
    "ax2.set_ylabel('Parameters (thousands)')\n",
    "ax2.set_title('Parameter Counts by Component')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"\\nDetailed Parameter Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "total = sum(param_counts)\n",
    "for component, count in sorted(param_stats.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = 100 * count / total\n",
    "    print(f\"{component:20s}: {count:8,} ({percentage:5.1f}%)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Total':20s}: {total:8,} (100.0%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps and Conclusions\n",
    "\n",
    "This notebook demonstrated the core implementation of the Mixture-of-Recursions model. Key observations:\n",
    "\n",
    "1. **Adaptive Computation**: The model successfully assigns different recursion depths to different tokens\n",
    "2. **Parameter Efficiency**: Shared recursive layers reduce parameter count compared to standard transformers\n",
    "3. **Training Stability**: The router loss helps balance computation across tokens\n",
    "\n",
    "### For Production Use:\n",
    "- Scale up model size and training data\n",
    "- Implement more sophisticated routing strategies\n",
    "- Add comprehensive evaluation on standard benchmarks\n",
    "- Optimize for inference speed with custom CUDA kernels\n",
    "\n",
    "### Potential Improvements:\n",
    "- Dynamic vocabulary routing\n",
    "- Hierarchical recursion patterns\n",
    "- Multi-modal extensions\n",
    "- Distillation from larger models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
